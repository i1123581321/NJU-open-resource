# Entropy, Relative Entropy, and Mutual Information

## Entropy

熵（Entropy）是对一个随机变量的不确定程度的度量

考虑离散型随机变量 $X$，及其 PMF $p(x) = \Pr\{X =x\}, x \in \mathcal{X}$，期望 $E_{p} g(x) = \sum g(x) p(x)$ ，定义一个随机变量 $X$ 的 entropy $H(X)$ 为
$$
H(X) = - \sum_{x \in \mathcal{X}} p(x)\log p(x) = \sum_{x \in \mathcal{X}} p(x) \log\frac{1}{p(x)} = E_{p} \log \frac{1}{p(x)}
$$
也可以写作 $H(p)$。entropy 仅仅与概率分布有关，而与随机变量具体的值无关

> lemma 2.1.1 $H(X) \geqslant 0$

## Joint Entropy and Conditional Entropy

考虑二维离散型随机变量 $(X,Y)$ 及其联合概率分布 $p(x, y)$，定义其联合熵（Joint Entropy）为
$$
H(X,Y) = -\sum_{x\in \mathcal{X}}\sum_{y \in \mathcal{Y}}p(x,y)\log p(x, y)
$$
同理，定义条件熵（Conditional Entropy）
$$
H(Y|X) = \sum_{x \in \mathcal{X}}p(x)H(Y|X = x) = -\sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}}p(x,y)\log p(y|x) = -E \log p(Y|X)
$$
由此得出链式法则（Chain Rule）
$$
H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
$$
及其推论
$$
H(X,Y|Z) = H(X|Z) + H(Y|X,Z)
$$

## Relative Entropy and Mutual Information

随机变量的 entropy 是描述随机变量需要的最小的信息量

相对熵（Relative Entropy）定义了两个随机分布间的距离
$$
D(p||q) = \sum_{x \in \mathcal{X}}p(x)\log \frac{p(x)}{q(x)} = E_{p}\log \frac{p(x)}{q(x)}
$$
考虑真实的分布 $p$ 和近似的分布 $q$，如果用分布 $p$ 编码表示该随机变量需要的信息量为 $H(p)$ ，用 $q$ 来编码表示需要的信息量就是 $H(p) + D(p||q)$

relative entropy 不是真正的距离，不对称且不满足三角不等式

互信息（Mutual Information）定义了一个随机变量包含多少另一个随机变量的信息
$$
I(X;Y) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x)p(y)} = D(p(x,y)||p(x)p(y)) = E \log \frac{p(x,y)}{p(x)p(y)} = I(Y;X)
$$

## Entropy and Mutual Information

根据定义可以得出
$$
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)
$$
即 $X$ 中 $Y$ 的信息和 $Y$ 中 $X$ 的信息一样多，同理
$$
I(X;X) = H(X) - H(X|X) = H(X)
$$
这也是熵被称为自信息的原因

![image-20200910225447588](https://i.loli.net/2020/09/13/D1tJ6XwE2hHLmYx.png)

## Chain Rules

chain rule for entropy
$$
H(X_{1}, X_{2}, \dots ,X_{n}) = \sum_{i=1}^{n}H(X_{i}|X_{i-1}, \dots, X_{1})
$$
可以定义条件互信息（Conditional Mutual Information）
$$
I(X;Y|Z) = H(X|Z) - H(X|Y,Z) = E \log \frac{p(X,Y|Z)}{p(X|Z)p(Y|Z)}
$$
chain rule for information
$$
I(X_{1}, X_{2}, \dots, X_{n};Y) = \sum_{i=1}^{n}I(X_{i};Y|X_{i-1}, \dots, X_{1})
$$
对于 relative entropy 也可以定义条件的版本
$$
D(p(y|x)||q(y|x)) = E \log \frac{p(Y|X)}{q(Y|X)}
$$
chain rule for relative entropy
$$
D(p(x, y) \| q(x, y))=D(p(x) \| q(x))+D(p(y | x) \| q(y | x))
$$
